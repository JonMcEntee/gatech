---
title: "Machine Learning - Assignment 2"
author: "Jonathan McEntee"
date: "10/3/2018"
output:
  pdf_document
#classoption: twocolumn
header-includes:
  \usepackage{tikz}
  \usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(extrafont)
library(grid)
library(gridExtra)
library(lattice)

adult_results <- read_csv("results/adult_results.csv")
adult_finals <- read_csv("results/adult_finals.csv")

count_ones_results <- read_csv("results/count_ones_results.csv")
count_ones_finals <- read_csv("results/count_ones_finals.csv")
count_ones_iterations <- read_csv("new_results/count_ones_finals.csv")

continuous_peaks_results <- read_csv("results/continuous_peaks_results.csv")
continuous_peaks_finals <- read_csv("results/continuous_peaks_finals.csv")
continuous_peaks_iterations <- read_csv("new_results/continuous_peaks_finals.csv")

flip_flop_results <- read_csv("results/flip_flop_results.csv")
flip_flop_finals <- read_csv("results/flip_flop_finals.csv")
flip_flop_iterations <- read_csv("new_results/flip_flop_finals.csv")

four_peaks_results <- read_csv("results/four_peaks_results.csv")
four_peaks_finals <- read_csv("results/four_peaks_finals.csv")
four_peaks_iterations <- read_csv("new_results/four_peaks_finals.csv")

plot_data <- function(df, title=NULL, subtitle=NULL,
                      xlab=NULL, ylab=NULL, invert=FALSE,
                      justification = c(1,0)) {
  
  if (invert) df <- df %>% mutate(score = 1/score)
  
  df %>%
    group_by(algorithm) %>%
    mutate(x = iteration / n()) %>%
    # filter(iteration %in% seq(1, n(), round(n()/10) - 1)) %>%
    ungroup() %>%
    ggplot(aes(x = x, y = score, color = algorithm)) +
    geom_line() +
    # geom_point() +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold"),
      legend.position = justification,
      legend.justification = justification,
      text = element_text(size = 10, family = "CMU Serif")
    ) +
    labs(
      title = title,
      subtitle = subtitle,
      x = xlab,
      y = ylab,
      color = NULL
    )
}

plot_time_iterations <- function(df, title) {
  new_data <- df %>%
    group_by(algorithm, bitstring_size) %>%
    summarize(
      score = mean(score),
      iterations = mean(iterations),
      training_time = mean(training_time)
    ) %>%
    ungroup()

  p1 <- new_data %>%
    ggplot(aes(x = bitstring_size, y = training_time, color = algorithm)) +
    geom_line() +
    geom_point() +
    theme_minimal() +
    theme(
      legend.position = c(0,1),
      legend.justification = c(0,1),
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      text = element_text(size = 10, family = "CMU Serif")
    ) +
    labs(
      color = NULL,
      x = NULL,
      y = "Training Time (Seconds)"
    )
  
  p2 <- new_data %>%
    ggplot(aes(x = bitstring_size, y = iterations, color = algorithm)) +
    theme_minimal() +
    geom_line() +
    geom_point() +
    guides(color = FALSE) +
    labs(x = NULL) +
    scale_y_continuous(position = "right") +
    theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      text = element_text(size = 10, family = "CMU Serif")
    ) +
    labs(
      y = "Iterations"
    )
    
  
  grid.arrange( grobs = list(p1, p2), nrow = 1,
                top = textGrob(title, x = .07, hjust = 0,
                               gp = gpar(fontfamily = "CMU Serif", fontsize = 12, fontface = "bold")),
                bottom = textGrob("Bitstring Size", gp = gpar(fontfamily = "CMU Serif", fontsize = 10)))
}
```

# Neural Network Weights With Randomized Optimization

The first experiment carried out, using the ABAGAIL library for java, applied three randomized optimization algorithms (randomized hill climbing, simulated annealing, and a genetic algorithm) to the problem of finding optimal weights for a neural network. Gradient descent was also performed for comparison. The neural network had fourteen inputs, an output node, and used a single hidden layer with five nodes. The objective of each algorithm was to minimize the sum of squares error between the categorical target value (0 or 1) and the output of the neural net. Put more precisely, the loss function for this experiment was:

$$error = \sum_{n=1}^m (t_i - o_i)^2$$
Where $t_i$ is the target value of the ith instance and $o_i$ was the output of the neural net for the ith instance.

## Adult Data Set

The Adult dataset has 32,000+ samples and features for each adult including their age, sex, race, etc. In my last experiment I subsampled the full dataset down to 6000 samples to save time on training. This time all 32,000+ samples were used. I also pre-processed the data: first selecting the 14 most relevant attributes, then scaling the data so all sample features were between -1 and 1, to prevent overdependence on variables with a larger range. The objective, as before, is to estimate whether the adult represented by the sample has an income greater than $50,000.

## Results

The four algorithms were trained for 1000 iterations each on the adult data.\newline

```{r, fig.width=6, fig.height=3, message=FALSE, warning=FALSE, fig.align="center"}
adult_results %>%
  plot_data(
    title = "Squared Error vs. Iterations on Adult Dataset",
    subtitle = "For Various Randomized Optimization Algorithms",
    xlab = "Iterations",
    ylab = "Error",
    invert = TRUE,
    justification = c(1,1))
```

The leftmost point for each algorithm on the graph above represents the sum of squares error (over the training data) after one iteration. By the time one iteration has passed, the gradient descent backpropagation algorithm has already reduced its squared error to 3933, lower than any of the randomized algorithms. It then quickly converges to about 1630, and (very, very slowly) continues to drop. The speed and accuracy with which gradient descent fits data is why it is often the first choice for training neural nets.

Randomized hill climbing ekes out second place. Though randomized hill climbing is completely unable to remove itself from local optima, that didn't seem to hurt it much in this case. The simulated annealing algorithm stands out the most when graphed. It rises and falls, although the rises are less high as the algorithm iterates. This is what we expect as the simulated annealing algorithm's temperature paramater cools over time, making the algorithm less likely to accept a value which raises the error. It is unable to converge in the same was as the other algorithms within 1000 iterations and has the highest error.

The genetic algorithm has a similar trajectory to the randomized algorithm in that it quickly declines and levels out, but it also apparently rises in some places. This is because the genetic algorithm doesn't necessarily hold on to its fittest member between generations. The algorithm generates a distribution which weights each member of its population by fitness, and then samples from that distribution. This is to prevent falling into local optima. However the genetic algorithm ultimately had a higher error than randomized hill climbing and a much longer training time.

```{r}
adult_finals %>%
  mutate(
    accuracy = paste0(accuracy %>% round(1), "%"),
    training_time = paste(round(training_time / 60, 1), "min"),
    test_time = paste(round(test_time, 1), "sec"),
    algorithm = recode(algorithm,
                       RHC = "Randomized Hill Climbing",
                       GA = "Genetic Algorithm",
                       SA = "Simulated Annealing",
                       GD = "Gradient Descent")
  ) %>%
  rename(
    Algorithm = algorithm,
    Correct = classified_correctly,
    Incorrect = classified_incorrectly,
    Accuracy = accuracy,
    `Training Time` = training_time,
    `Test Time` = test_time
  ) %>%
  knitr::kable()
```

# The "Count Ones" Problem

The "count ones" problem, as the name suggests, is an optimization problem on bitstrings where the evaluation function simply counts the number of ones in the bitstring. The more ones, the higher its fitness score. For example, if the bit strings were 5 bits long, then a string `10111` would be given a score of 4. The order of the bits plays no importance, so `11101` would recieve the same score.

Because the evaluation function doesn't account for the order of the bits, there are no conditional dependencies between them. So we would expect MIMIC and genetic algorithms to have no particular advantage against truly structureless optimization algorithms like randomized hill climbing and simulated annealing.

There is also only one optimum in this problem, so the randomized hill climbing algorithm will not get "stuck" at local optima. What do I mean by this? Consider the randomized hill climbing algorithm's neighbor function in ABAGAIL. The neighbor of the randomized hill climbing algorithm's current bitsrting is that same bitstring but with one randomly selected bit changed. So the string `10010` has the neighbors `00010`, `11010`, `10110`, `10000`, and `10011`. This means that for any suboptimal bit string, there will always be a neighbor that improves the fitness score. With no local optimums to be trapped in, simulated annealing shouldn't have any advantage over randomized hill climbing either.

I ran all four algorithms against solving the count ones problem on bitstrings of length 20, 30, 40, 50, and 60, ten times each. Algorithms which saw no increase in score for more than 2000 iterations recieved a random reset. The algorithms were run until they reached a perfect fitness score.

Simulated annealing's temperature was initially set to 100 with a cooling factor of 0.95. The genetic algorithm had a population of 200 bitstrings. Each iteration, 20 bitstrings in the new population were generated from a discrete uniform crossover function. Another 20 were put through a mutation function that was equivalent to the random step used in simulated annealing and randomized hill climbing. MIMIC was set to take 50 samples from its distribution and keep 10 each iteration.

```{r, fig.width=6, fig.height=3, message=FALSE, fig.align="center"}
plot_time_iterations(count_ones_iterations,
                     title = 'Iteration/Time on "Count Ones" To Reach Perfect Score')
```

In terms of training time, randomized hill climbing and simulated annealing reached a perfect score in the smallest amount of time on larger strings. the genetic algorithm, and MIMIC were third and fourth fastest respectively. This is what we expected. Both MIMIC and the genetic algorithm run slower, but have the advantage of being able to capture the structure of a bitstring. Here there is no structure, and the additional time cost required to capture structure becomes a disadvantage.

However MIMIC required the least number of iterations to find the perfect score, followed by randomized hill climbing, simulated annealing, and the genetic algorithm. If the evaluation function had a higher time cost, MIMIC would theoretically be able to beat out randomized hill climbing and simulated annealing.

To get a better understanding of how these algorithms trained, iteration by iteration, I ran the algorithms again to solve a 100 length bitstring, 100 times. Randomized hill climbing and simulated annealing were run for 1000 iterations, the genetic algorithm was run for 300 iterations, and MIMIC 100.

```{r, fig.width=6, fig.height=3, message=FALSE, fig.align="center"}
count_ones_results %>%
  filter(run_num == 32) %>%
  plot_data(
    title = 'The "Count Ones" Problem',
    subtitle = "(Run 32 Of 100)",
    xlab = "Iterations",
    ylab = "Score")
```

```{r}
count_ones_finals %>%
  group_by(algorithm) %>%
  summarize(
    score = mean(score),
    iterations = mean(iterations),
    training_time = mean(training_time)
  ) %>%
  ungroup() %>%
  mutate(
    training_time = paste(round(training_time, 1), "sec"),
    algorithm = recode(algorithm,
                       RHC = "Randomized Hill Climbing",
                       GA = "Genetic Algorithm",
                       SA = "Simulated Annealing")
  ) %>%
  rename(
    Algorithm = algorithm,
    Score = score,
    `Training Time` = training_time
  ) %>%
  arrange(desc(Score)) %>%
  knitr::kable()
```



# The "Continuous Peaks" Problem

The continuous peaks problem is another bitstring problem. Unlike count ones, continuous peaks has an evaluation function that increases with the longest continuous string of ones or zeros. If the longest string of ones and the longest string of zeros both are longer than some parameter $T = N/10$ where $N$ is the length of the bitstring, then a bonus of $N$ points will be added to the score. Put precisely:

$$score = max(num_0(b), num_1(b)) + R(b)$$

Where $num_0(b)$ and $num_1(b)$ is the length of the longest continuous string of zeros and ones in bitstring b, and

$$R(b) =  \begin{cases} 
      length(b) & num_0(b) > T\ and\ num_1(b) > T \\ 
      0 & otherwise
   \end{cases}$$ \newline
   
```{r, fig.width=6, fig.height=3, message=FALSE, warning=FALSE, fig.align="center"}
g <- expand.grid(x = seq(1, 10, 0.5), y = seq(1, 10, 0.5))

continous_peaks_lower <- function(x, y) {
  if (x > 2 && y > 2) {
    return(max(x, y) + 10)
  } else if (x + y <= 10) {
    return(max(x, y))
  } else {
    return(NA)
  }
}

continuous_peaks <- function(x, y) {
  z <- double(length = length(x))
  for (i in seq(length(x))) {
    z[i] <- continous_peaks_lower(x[i], y[i])
  }
  
  return(z)
}

g$z <- continuous_peaks(g$x, g$y)

textify <- function(title, font_size = 12, font_face = NULL) {
  textGrob(title, gp = gpar(fontfamily = "CMU Serif", fontsize = font_size, fontface = font_face))
}

wireframe(z ~ x * y, data = g,
          scales = list(arrows = FALSE),
          drape = TRUE, colorkey = TRUE,
          screen = list(z = 30, x = -60),
          xlab = textify("zeros"),
          ylab = textify("ones"),
          zlab = textify("fit"),
          main = textGrob("Continous Peaks Evaluation Function", x = .03, hjust = 0.1,
                          gp = gpar(fontfamily = "CMU Serif", fontsize = 14, fontface = "bold")))

```

I again ran all four

```{r, fig.width=6, fig.height=3, message=FALSE, fig.align="center"}
continuous_peaks_results %>%
  filter(run_num == 0, bitstring_size == 40) %>%
  plot_data(
    title = "The 'Continuous Peaks' Problem",
    xlab = "Iterations",
    ylab = "Error")
```



```{r, fig.width=6, fig.height=3, message=FALSE, fig.align="center"}
plot_time_iterations(continuous_peaks_iterations,
                     title = 'Iteration/Time on "Continuous Peaks" To Reach Perfect Score')
```

```{r}
continuous_peaks_finals %>%
  group_by(algorithm) %>%
  summarize(
    score = mean(score),
    iterations = mean(iterations),
    training_time = mean(training_time)
  ) %>%
  ungroup() %>%
  mutate(
    training_time = paste(round(training_time, 1), "sec"),
    algorithm = recode(algorithm,
                       RHC = "Randomized Hill Climbing",
                       GA = "Genetic Algorithm",
                       SA = "Simulated Annealing")
  ) %>%
  rename(
    Algorithm = algorithm,
    Score = score,
    `Training Time` = training_time
  ) %>%
  arrange(desc(Score)) %>%
  knitr::kable()
```

# Solving The "Flip Flop" Problem With Randomized Optimization

In the flip flop problem, the fitness function rewards bitstrings that alternate between zero and one. For example, `01010` is a global optimum for $N=5$. This gives the bits value relative to the bits surrounding it. A one preceded and followed by a one has zero fitness, but a one preceded and followed by a zero has two fitness.

This is the kind of structure that should be captured well by a genetic algorithm and especially MIMIC. Furthermore, unlike count ones, flip flop has local optima that an algorithm like randomized hill climbing could get trapped in. `10110` has no neighboring bitstrings which increase the fitness. These local optimums are avoided by simulated annealing, which can roll over them when its temperature is high.

I ran all four algorithms, set to random restart after 2000 iterations with no improvement. The randomized hill climbing algorithm was not able to converge to the perfect score within 100,000 iterations and thus is removed from the results below.

```{r, fig.width=6, fig.height=3, message=FALSE, fig.align="center"}
plot_time_iterations(flip_flop_iterations %>% filter(algorithm != "RHC"),
                     title = 'Iteration/Time on "Flip Flop" To Reach Perfect Score')
```

In terms of training time, the simulated annealing algorithm beat out both the genetic algorithm and MIMIC. While MIMIC wins in iterations, as it did with the count ones problem, the cost of checking the evaluation function is so low that simulated annealing can blaze through thousands of iterations a second. To get a more detailed look at how the algorithms trained, I again looked at 

```{r, fig.width=6, fig.height=3, message=FALSE, fig.align="center"}
flip_flop_results %>%
  filter(run_num == 4) %>%
  plot_data(
    title = "The 'Flip Flop' Problem",
    xlab = "Iterations",
    ylab = "Score")
```

```{r, fig.width=6, fig.height=3, message=FALSE, fig.align="center"}
four_peaks_results %>%
  filter(run_num == 14) %>%
  plot_data(
    title = "The 'Four Peaks' Problem",
    xlab = "Iterations",
    ylab = "Error")
```

```{r, fig.width=6, fig.height=3, message=FALSE, fig.align="center"}
plot_time_iterations(four_peaks_iterations,
                     title = 'Iteration/Time on "Four Peaks" To Reach Perfect Score')
```

```{r}
four_peaks_finals %>%
  group_by(algorithm) %>%
  summarize(
    score = mean(score),
    iterations = mean(iterations),
    training_time = mean(training_time)
  ) %>%
  ungroup() %>%
  mutate(
    training_time = paste(round(training_time, 1), "sec"),
    algorithm = recode(algorithm,
                       RHC = "Randomized Hill Climbing",
                       GA = "Genetic Algorithm",
                       SA = "Simulated Annealing")
  ) %>%
  rename(
    Algorithm = algorithm,
    Score = score,
    `Training Time` = training_time
  ) %>%
  arrange(desc(Score)) %>%
  knitr::kable()
```
