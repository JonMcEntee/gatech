---
title: "Machine Learning - Assignment 2"
author: "Jonathan McEntee"
date: "10/3/2018"
output:
  pdf_document
#classoption: twocolumn
header-includes:
  \usepackage{tikz}
  \usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(extrafont)
adult_results <- read_csv("results/adult_results.csv")
adult_finals <- read_csv("results/adult_finals.csv")
```

# Neural Network Weights With Randomized Optimization

The first experiment carried out, using the ABAGAIL library for java, applied three randomized optimization algorithms (randomized hill climbing, simulated annealing, and a genetic algorithm) to the problem of finding optimal weights for a neural network. Gradient descent was also performed for comparison. The neural network had fourteen inputs, an output node, and used a single hidden layer with five nodes. The objective of each algorithm was to minimize the sum of squares error between the categorical target value (0 or 1) and the output of the neural net. Put more precisely, the loss function for this experiment was:

$$error = \sum_{n=1}^m (t_i - o_i)^2$$
Where $t_i$ is the target value of the ith instance and $o_i$ was the output of the neural net for the ith instance.

## Adult Data Set

The Adult dataset has 32,000+ samples and features for each adult including their age, sex, race, etc. In my last experiment I subsampled the full dataset down to 6000 samples to save time on training. This time all 32,000+ samples were used. I also pre-processed the data: first selecting the 14 most relevant attributes, then scaling the data so all sample features were between -1 and 1, to prevent overdependence on variables with a larger range. The objective, as before, is to estimate whether the adult represented by the sample has an income greater than $50,000.

## Results

The four algorithms were trained for 1000 iterations each on the adult data.\newline

```{r, fig.width=6, fig.height=3, message=FALSE, warning=FALSE, fig.align="center"}
adult_results %>%
  mutate(score = 1/score) %>%
  group_by(algorithm) %>%
  mutate(x = seq(1:1000)) %>%
  ungroup() %>%
  filter(x %in% seq(1, 1000, 49)) %>%
  ggplot(aes(x = x, y = score, color = algorithm)) +
  geom_line() +
  geom_point() +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    legend.position = c(1,1),
    legend.justification = c(1,1),
    text = element_text(size = 10, family = "CMU Serif")
  ) +
  labs(
    title = "Squared Error vs. Iterations on Adult Dataset",
    subtitle = "For Various Randomized Optimization Algorithms",
    x = "Iterations",
    y = "Error",
    color = NULL
  )
```

The leftmost point for each algorithm on the graph above represents the sum of squares error (over the training data) after one iteration. By the time one iteration has passed, the gradient descent backpropagation algorithm has already reduced its squared error to 3933, lower than any of the randomized algorithms. It then quickly converges to about 1630, and (very, very slowly) continues to drop. The speed and accuracy with which gradient descent fits data is why it is often the first choice for training neural nets.

Randomized hill climbing ekes out second place. Though randomized hill climbing is completely unable to remove itself from local optima, that didn't seem to hurt it much in this case. The simulated annealing algorithm stands out the most when graphed. It rises and falls, although the rises are less high as the algorithm iterates. This is what we expect as the simulated annealing algorithm's temperature paramater cools over time, making the algorithm less likely to accept a value which raises the error. It is unable to converge in the same was as the other algorithms within 1000 iterations and has the highest error.

The genetic algorithm has a similar trajectory to the randomized algorithm in that it quickly declines and levels out, but it also apparently rises in some places. This is because the genetic algorithm doesn't necessarily hold on to its fittest member between generations. The algorithm generates a distribution which weights each member of its population by fitness, and then samples from that distribution. This is to prevent falling into local optima. However the genetic algorithm ultimately had a higher error than randomized hill climbing and a much longer training time.

```{r}
adult_finals %>%
  mutate(
    accuracy = paste0(accuracy %>% round(1), "%"),
    training_time = paste(round(training_time / 60, 1), "min"),
    test_time = paste(round(test_time, 1), "sec"),
    algorithm = recode(algorithm,
                       RHC = "Randomized Hill Climbing",
                       GA = "Genetic Algorithm",
                       SA = "Simulated Annealing",
                       GD = "Gradient Descent")
  ) %>%
  rename(
    Algorithm = algorithm,
    Correct = classified_correctly,
    Incorrect = classified_incorrectly,
    Accuracy = accuracy,
    `Training Time` = training_time,
    `Test Time` = test_time
  ) %>%
  knitr::kable()
```

# Solving The "Flip Flop" Problem With Randomized Optimization

