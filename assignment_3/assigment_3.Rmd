---
title: "Machine Learning - Assignment 3"
author: "Jonathan McEntee"
date: "10/29/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(extrafont)
library(tidyverse)
library(tidyquant)
library(forcats)

nn_baseline_mean <- read_csv("nn_baseline_2_adult.csv") %>%
  select(num_layers, mean_test_score, mean_train_score) %>%
  gather(score_type, score, mean_test_score, mean_train_score) %>%
  mutate(score_type = recode(score_type, mean_test_score = "test", mean_train_score = "train"))

nn_baseline_sd <- read_csv("nn_baseline_2_adult.csv") %>%
  select(num_layers, std_test_score, std_train_score) %>%
  gather(score_type, score_sd, std_test_score, std_train_score) %>%
  mutate(score_type = recode(score_type, std_test_score = "test", std_train_score = "train"))

nn_baseline_adult <- left_join(nn_baseline_mean, nn_baseline_sd, by = c("num_layers", "score_type")) %>%
  mutate(
    lower_bound = score - 1.96 * score_sd,
    upper_bound = score + 1.96 * score_sd
  )

tsne_clusters_adult <- read_csv("tsne_clusters_adult.csv")

two_d_adult <- read_csv("two_d_adult.csv")

cluster_metrics_adult <- read_csv("cluster_metrics_adult.csv") %>%
  select(k, model, adj_mutual_info, adj_rand, silhouette, v_measure) %>%
  gather(score_type, score, adj_mutual_info:v_measure)
```

# The Adult Dataset

The Adult dataset has 32,000+ samples and features for each adult including their age, sex, race, etc. Our objective is to predict whether each adult has an income of greater or less than $50,000, making this a binary classification problem. In assignment 1, using the scikit-learn library for Python, I ran a 6000 instance subsample of the dataset on a neural network classifier with a single layer of 90 nodes. I was able to achieve an accuracy of 79.3% on my test set. In this analysis I again subsampled the data to 6000 instances.

The adult dataset has dozens of one-hot encoded variables, which is problematic for clustering algorithms that assume euclidean distance. To counter this I reduced the number of features to the 40 most important (as determined by a random forest classifier), and scaled the data using scikit-learn's StandardScaler so that no feature would be weighted more heavily than the others.

To have a baseline to compare the results of my cluster analysis to, I reran the neural network on this transformed dataset. I also re-performed my complexity analysis to insure the optimal number of nodes in my single layer.

&nbsp;

```{r, fig.width=6, fig.height=2.5, message=FALSE, warning=FALSE, fig.align="center"}
nn_baseline_adult %>%
  mutate(
    score_type = recode(score_type,
                        test = "Cross Validation",
                        train = "Train")
  ) %>%
  ggplot(aes(x = num_layers)) +
  geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound, fill = score_type), alpha = 0.3) +
  geom_line(aes(y = score, color = score_type)) +
  geom_point(aes(y = score, color = score_type)) +
  theme_minimal() +
  theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      text = element_text(size = 10, family = "CMU Serif"),
      plot.title = element_text(face = "bold"),
      legend.position = c(0,1),
      legend.justification =  c(0,1),
    ) +
  labs(
    title = "Neural Network Learning Curve",
    x = "Number Of Nodes",
    y = "Accuracy",
    color = NULL,
    fill = NULL
    )
```

As the cross-validation score did not seem to improve with more nodes, I reduced the number of nodes to 5.

## Cluster Analysis With K-Means and Expectation Maximization

Using the scikit-learn library for Python, I ran the feature-reduced, subsampled data set through both a k-means and an expectation maximization algorithm. To evaluate the quality of the clustering algorithms for different numbers of clusters, I applied several performance metrics. The descriptions below are taken from the scikit-learn documentation.

- **Rand Score:** A metric for supervised learning. Defined as $RI = (a + b)/{n_{pairs}}$, where $a$ is the number of pairs of that are in the same cluster that also have the same class, $b$ is the number of pairs of elements in different clusters that have a different class, and $n_{pairs}$ is the total number of possible unordered pairs. The score is bounded between -1 and 1, and a higher score is seen as a better clustering. We actually use an "adjusted" rand score which assigns random labelings close to 0.

- **Mutual Information Score:** A metric for supervised learning. Defined as the mutual information between the clustering of the data and the true classification of the data. Probability for each cluster or class $i \in C$ is $|C_i|/|C|$. It is bounded between 0 and 1 with a higher score representating higher mutual information, and therefore a clustering scheme that represents the underlying classes. The mutual information score as is has a tendency to go up with the number of clusters. Therefore we use an adjusted mutual information score that does not have this tendency.

- **V Measure:** A supervised learning metric. The harmonic mean of a clustering scheme's "homogeneity" and "completeness". Where homogeneity is a measure of each cluster only containing one class, and completeness is a measure of all members of a given class being assigned to the same cluster. Bounded between 0 and 1, with higher scores indicating better clustering schemes.

- **Silhouette Coefficient:** An unsupervised learning metric. for each sample a silhouette coefficient $s = (b - a)/max(a,b)$ is calculated. Where $a$ is the mean distance between the sample and all other points of the same class, and $b$ is the mean distance between the sample and all other points in the next nearest cluster. It is bounded between -1 and 1, with higher scores indicating denser clustering.

These scores were calculated for both k-means, and expectation maximization, for different numbers of means ranging from 2 to 50.  
  
&nbsp;

```{r, fig.width=6, fig.height=2.5, message=FALSE, warning=FALSE, fig.align="center"}
cluster_metrics_adult %>%
  filter(k != 100) %>%
  mutate(
    score_type = recode(score_type,
                        adj_mutual_info = "Adj. Mutual Info",
                        adj_rand = "Adj. Rand Score",
                        silhouette = "Silhouette Coeff.",
                        v_measure = "V Measure"),
    model = recode(model,
                   em = "Expectation Maximization",
                   kmeans = "K Means")
  ) %>%
  ggplot(aes(x = k, y = score, color = score_type)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ model) +
  theme_minimal() +
  theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      text = element_text(size = 10, family = "CMU Serif"),
      plot.title = element_text(face = "bold")
    ) +
  labs(
    title = "Cluster Quality vs Number of Clusters",
    x = "Number Of Means",
    y = "Score",
    color = NULL
    )
```

All of our supervised learning metrics show two clusters as the optimal clustering scheme for both k-means and expectation maximization. The silhouette coefficient rises as the number of clusters increases. This indicates that with a higher number of clusters, the clusters become more dense and distant from each other. The falling rand score, mutual information, and v measure on the other hand, indicate that a larger number of clusters doesn't improve the separation of the base classes. Given that our t-SNE analysis showed a single cluster, this result makes sense.

## Dimensionality Reduction Algorithms

Our feature reduced adult dataset still has 40 features, which makes it difficult to visualize. Dimensionality reduction algorithms allow us to visualize high dimensional datasets as two dimensional plots. The dataset is visualized below in two dimensions using the t-SNE algorithm.

&nbsp;

```{r, fig.width=6, fig.height=3.5, message=FALSE, warning=FALSE, fig.align="center"}
tsne_clusters_adult %>%
    ggplot(aes(x = X1, y = X2, color = y)) +
    geom_point(alpha = 0.1) +
    facet_wrap(~ perplexity, nrow = 2) +
    theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold", hjust = 0.4),
        axis.text.x = element_blank(),
        axis.text.y = element_blank()
      ) +
    labs(
      title = "tSNE Dimensionality Reduction Of The Adult Dataset",
      x = "X1",
      y = "X2",
      color = NULL
      ) +
    guides(color = FALSE)
```

t-SNE is a dimensionality reduction algorithm that preserves local pairwise distances and is helpful for pulling out cluster structure in high-dimensional data. Unfortunately our t-SNE graph seems to suggest only one cluster in euclidean space, which will be very problematic for our cluster analysis.

t-SNE is not the only dimensionality reduction algorithm we have though, below I have projected the data into two dimensions with four separate algorithms: principle component analysis, independent component analysis, randomized projection, and truncated singular value decomposition.

&nbsp;

```{r, fig.width=6, fig.height=4, message=FALSE, warning=FALSE, fig.align="center"}
two_d_adult %>%
    mutate(
    algorithm = recode(algorithm,
                        pca = "PCA",
                        ica = "ICA",
                        rp = "Randomized Projection",
                        svd = "Truncated SVD")) %>%
    ggplot(aes(x = X1, y = X2, color = y)) +
    geom_point(alpha = 0.5) +
    facet_wrap(~ algorithm, scales = "free") +
      theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold", hjust = 0.4),
        axis.text.x = element_blank(),
        axis.text.y = element_blank()
      ) +
    labs(
      title = "Various Dimensionality Reduction Strategies on Adult",
      x = "X1",
      y = "X2",
      color = NULL
      ) +
    guides(color = FALSE)
```

Interestingly, ICA, PCA, and truncated SVD all produce the same results. Truncated SVD is an algorithm very similar to PCA













