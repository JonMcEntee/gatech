---
title: "Machine Learning - Assignment 3"
author: "Jonathan McEntee"
date: "10/29/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(extrafont)
library(tidyverse)
library(tidyquant)
library(forcats)

nn_baseline_mean <- read_csv("nn_baseline_2_adult.csv") %>%
  select(num_layers, mean_test_score, mean_train_score) %>%
  gather(score_type, score, mean_test_score, mean_train_score) %>%
  mutate(score_type = recode(score_type, mean_test_score = "test", mean_train_score = "train"))

nn_baseline_sd <- read_csv("nn_baseline_2_adult.csv") %>%
  select(num_layers, std_test_score, std_train_score) %>%
  gather(score_type, score_sd, std_test_score, std_train_score) %>%
  mutate(score_type = recode(score_type, std_test_score = "test", std_train_score = "train"))

nn_baseline_adult <- left_join(nn_baseline_mean, nn_baseline_sd, by = c("num_layers", "score_type")) %>%
  mutate(
    lower_bound = score - 1.96 * score_sd,
    upper_bound = score + 1.96 * score_sd
  )

tsne_clusters_adult <- read_csv("tsne_clusters_adult.csv")

two_d_adult <- read_csv("two_d_adult.csv")

dim_reduction_adult <- read_csv("dim_reduction_adult.csv")

pca_explained_variance_adult <- read_csv("pca_explained_variance_adult.csv") %>%
  mutate(explained_variance = explained_variance / sum(explained_variance),
         cumulative_variance = cumsum(explained_variance))

ica_kurtosis_adult <- read_csv("ica_kurtosis_adult.csv")

reconstruction_error_adult <- read_csv("reconstruction_error_adult.csv")

cluster_metrics_adult <- read_csv("cluster_metrics_adult.csv") %>%
  select(k, model, adj_mutual_info, adj_rand, silhouette, v_measure) %>%
  gather(score_type, score, adj_mutual_info:v_measure)
```

# The Adult Dataset

The Adult dataset has 32,000+ samples and features for each adult including their age, sex, race, etc. Our objective is to predict whether each adult has an income of greater or less than $50,000, making this a binary classification problem. In assignment 1, using the scikit-learn library for Python, I ran a 6000 instance subsample of the dataset on a neural network classifier with a single layer of 90 nodes. I was able to achieve an accuracy of 79.3% on my test set. In this analysis I again subsampled the data to 6000 instances.

The adult dataset has dozens of one-hot encoded variables, which is problematic for clustering algorithms that assume euclidean distance. To counter this I reduced the number of features to the 40 most important (as determined by a random forest classifier), and scaled the data using scikit-learn's StandardScaler so that no feature would be weighted more heavily than the others.

To have a baseline to compare the results of my cluster analysis to, I reran the neural network on this transformed dataset. I also re-performed my complexity analysis to insure the optimal number of nodes in my single layer.

&nbsp;

```{r, fig.width=6, fig.height=2.5, message=FALSE, warning=FALSE, fig.align="center"}
nn_baseline_adult %>%
  mutate(
    score_type = recode(score_type,
                        test = "Cross Validation",
                        train = "Train")
  ) %>%
  ggplot(aes(x = num_layers)) +
  geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound, fill = score_type), alpha = 0.3) +
  geom_line(aes(y = score, color = score_type)) +
  geom_point(aes(y = score, color = score_type)) +
  theme_minimal() +
  theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      text = element_text(size = 10, family = "CMU Serif"),
      plot.title = element_text(face = "bold"),
      legend.position = c(0,1),
      legend.justification =  c(0,1)
    ) +
  labs(
    title = "Neural Network Learning Curve",
    x = "Number Of Nodes",
    y = "Accuracy",
    color = NULL,
    fill = NULL
    )
```

As the cross-validation score did not seem to improve with more nodes, I reduced the number of nodes to 5.

## Cluster Analysis With K-Means and Expectation Maximization

Using the scikit-learn library for Python, I ran the feature-reduced, subsampled data set through both a k-means and an expectation maximization algorithm. To evaluate the quality of the clustering algorithms for different numbers of clusters, I applied several performance metrics. The descriptions below are taken from the scikit-learn documentation.

- **Rand Score:** A metric for supervised learning. Defined as $RI = (a + b)/{n_{pairs}}$, where $a$ is the number of pairs of that are in the same cluster that also have the same class, $b$ is the number of pairs of elements in different clusters that have a different class, and $n_{pairs}$ is the total number of possible unordered pairs. The score is bounded between -1 and 1, and a higher score is seen as a better clustering. We actually use an "adjusted" rand score which assigns random labelings close to 0.

- **Mutual Information Score:** A metric for supervised learning. Defined as the mutual information between the clustering of the data and the true classification of the data. Probability for each cluster or class $i \in C$ is $|C_i|/|C|$. It is bounded between 0 and 1 with a higher score representating higher mutual information, and therefore a clustering scheme that represents the underlying classes. The mutual information score as is has a tendency to go up with the number of clusters. Therefore we use an adjusted mutual information score that does not have this tendency.

- **V Measure:** A supervised learning metric. The harmonic mean of a clustering scheme's "homogeneity" and "completeness". Where homogeneity is a measure of each cluster only containing one class, and completeness is a measure of all members of a given class being assigned to the same cluster. Bounded between 0 and 1, with higher scores indicating better clustering schemes.

- **Silhouette Coefficient:** An unsupervised learning metric. for each sample a silhouette coefficient $s = (b - a)/max(a,b)$ is calculated. Where $a$ is the mean distance between the sample and all other points of the same class, and $b$ is the mean distance between the sample and all other points in the next nearest cluster. It is bounded between -1 and 1, with higher scores indicating denser clustering.

These scores were calculated for both k-means, and expectation maximization, for different numbers of means ranging from 2 to 50.  
  
&nbsp;

```{r, fig.width=6, fig.height=2.5, message=FALSE, warning=FALSE, fig.align="center"}
cluster_metrics_adult %>%
  filter(k != 100) %>%
  mutate(
    score_type = recode(score_type,
                        adj_mutual_info = "Adj. Mutual Info",
                        adj_rand = "Adj. Rand Score",
                        silhouette = "Silhouette Coeff.",
                        v_measure = "V Measure"),
    model = recode(model,
                   em = "Expectation Maximization",
                   kmeans = "K Means")
  ) %>%
  ggplot(aes(x = k, y = score, color = score_type)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ model) +
  theme_minimal() +
  theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank(),
      text = element_text(size = 10, family = "CMU Serif"),
      plot.title = element_text(face = "bold")
    ) +
  labs(
    title = "Cluster Quality vs Number of Clusters",
    x = "Number Of Means",
    y = "Score",
    color = NULL
    )
```

All of our supervised learning metrics show two clusters as the optimal clustering scheme for both k-means and expectation maximization. The silhouette coefficient rises as the number of clusters increases. This indicates that with a higher number of clusters, the clusters become more dense and distant from each other. The falling rand score, mutual information, and v measure on the other hand, indicate that a larger number of clusters doesn't improve the separation of the base classes. Given that our t-SNE analysis showed a single cluster, this result makes sense.

## Dimensionality Reduction Algorithms

Our feature reduced adult dataset still has 40 features, which makes it difficult to visualize. Dimensionality reduction algorithms allow us to visualize high dimensional datasets as two dimensional plots. The dataset is visualized below in two dimensions using the t-SNE algorithm.

&nbsp;

```{r, fig.width=6, fig.height=3.5, message=FALSE, warning=FALSE, fig.align="center"}
tsne_clusters_adult %>%
    ggplot(aes(x = X1, y = X2, color = y)) +
    geom_point(alpha = 0.1) +
    facet_wrap(~ perplexity, nrow = 2) +
    theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold", hjust = 0.4),
        axis.text.x = element_blank(),
        axis.text.y = element_blank()
      ) +
    labs(
      title = "tSNE Dimensionality Reduction Of The Adult Dataset",
      x = "X1",
      y = "X2",
      color = NULL
      ) +
    guides(color = FALSE)
```

t-SNE is a dimensionality reduction algorithm that preserves local pairwise distances and is helpful for pulling out cluster structure in high-dimensional data. Unfortunately our t-SNE graph seems to suggest only one cluster in euclidean space, which will be very problematic for our cluster analysis.

t-SNE is not the only dimensionality reduction algorithm we have though, below I have projected the data into two dimensions with four separate algorithms: principle component analysis, independent component analysis, randomized projection, and truncated singular value decomposition.

&nbsp;

```{r, fig.width=6, fig.height=4, message=FALSE, warning=FALSE, fig.align="center"}
two_d_adult %>%
    mutate(
    algorithm = recode(algorithm,
                        pca = "PCA",
                        ica = "ICA",
                        rp = "Randomized Projection",
                        svd = "Truncated SVD")) %>%
    ggplot(aes(x = X1, y = X2, color = y)) +
    geom_point(alpha = 0.5) +
    facet_wrap(~ algorithm, scales = "free") +
    theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold", hjust = 0.4),
        axis.text.x = element_blank(),
        axis.text.y = element_blank()
      ) +
    labs(
      title = "Various Dimensionality Reduction Strategies on Adult",
      x = "X1",
      y = "X2",
      color = NULL
      ) +
    guides(color = FALSE)
```

Interestingly, ICA, PCA, and truncated SVD all produce the same results (ICA is flipped and rotated, but its structure is the same). ICA and PCA producing the same result indicates that our data is of a guassian distribution. In other words, all information about this data set and the distribution of its instances is captured by the covariance matrix used by PCA. There's nothing for the ICA to find that the PCA cannot, and so they produce the same result. The truncated SVD being identical to PCA also makes sense, since the data is standardized to $\mu = 0$ and $\sigma^2 = 1$, and truncated SVD is the same as PCA when the data is centered at the mean.

In the two PCA components graphed, we can see two clusters, each predominantly of one class, which suggests that two clusters might be the optimal choice for grouping the data. This clustering scheme is also supported by results of our initial clustering.

&nbsp;

```{r, fig.width=6, fig.height=1.8, message=FALSE, warning=FALSE, fig.align="center"}
pca_explained_variance_adult %>%
    ggplot(aes(x = component, y = explained_variance)) +
    geom_bar(stat = "identity") +
    theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold")
      ) +
    labs(
      title = "Amount Of Variance Explained By Principal Component",
      x = "Principal Component",
      y = "Explained Variance",
      color = NULL
      ) +
    scale_y_continuous(labels = scales::percent)
```

Examining the eigenvalues of our PCA we see that 14% of the variance in the data can be explained with two components, 50% can be explained with 13 components, and 95% can be explained with 37 components. Reducing the dimension of the data may improve the neural net's ability to train on the data, while still preserving most of the information within it.

&nbsp;

```{r, fig.width=6, fig.height=1.8, message=FALSE, warning=FALSE, fig.align="center"}
ica_kurtosis_adult %>%
    filter(n < 2 | n > 5) %>%
    ggplot(aes(x = n, y = kurtosis)) +
    geom_line() +
    geom_point(shape = "square") +
    theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold")
      ) +
    labs(
      title = "Kurtosis vs Number of ICA Components",
      x = "Number of Components",
      y = "Kurtosis",
      color = NULL
      )
```

For ICA the kurtosis, a measure of non-guassianity, increases monotonically with the number of components. This suggests the algorithm's components become more independent as the number of components increases.

&nbsp;

```{r, fig.width=6, fig.height=1.8, message=FALSE, warning=FALSE, fig.align="center"}
reconstruction_error_adult %>%
    group_by(n) %>%
    summarize(
      std = sd(mean, na.rm = TRUE),
      mean = mean(mean, na.rm = TRUE)
    ) %>%
    ungroup() %>%
    mutate(
      bottom = mean - 1.96 * std,
      top = mean + 1.96 * std
    ) %>%
    ggplot(aes(x = n)) +
    geom_ribbon(aes(ymin = bottom, ymax = top), alpha = 0.5) +
    geom_line(aes(y = mean)) +
    geom_point(aes(y = mean), shape = "square") +
    theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold")
      ) +
    labs(
      title = "Reconstruction Error For Different Component Sizes",
      subtitle = "Averaged Over 20 Random Projections",
      x = "Number of Components",
      y = "Reconstruction Error",
      color = NULL
      )
```

The random projections were, unsurprisingly able to capture the data better with a higher number of components. The error plateaus at around 10 components.

## Clustering Projected Data

After projecting our data using a dimensionality reduction algorithm, we can try clustering again to see if the clusters have changed.

&nbsp;

```{r, fig.width=6, fig.height=3.2, message=FALSE, warning=FALSE, fig.align="center"}
dim_reduction_adult %>%
    arrange(n, k) %>%
    mutate(n = as.factor(n), k = as.factor(k),
           clusterer = recode(clusterer,
                              em = "EM",
                              kmeans = "K Means"),
           algorithm = recode(algorithm,
                              rp = "Random Proj.",
                              ica = "ICA",
                              pca = "PCA",
                              svd = "SVD"),
           title = paste(clusterer, ":", algorithm)) %>%
    ggplot(aes(x = n, y = k)) +
    geom_tile(aes(fill = silhouette)) +
    facet_wrap(~ title, nrow = 2) +
    theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold")
      ) +
    labs(
      title = "Silhouette Coefficient For Different Hyperparameters",
      subtitle = "Lighter Tiles Indicate Higher Score",
      x = "Number of Components",
      y = "Number of Clusters",
      color = NULL
      ) +
    guides(fill = FALSE) +
    scale_fill_gradient(low = "darkgreen", high = "springgreen3")
```

&nbsp;

```{r, fig.width=6, fig.height=3.2, message=FALSE, warning=FALSE, fig.align="center"}
dim_reduction_adult %>%
    arrange(n, k) %>%
    mutate(n = as.factor(n), k = as.factor(k),
           clusterer = recode(clusterer,
                              em = "EM",
                              kmeans = "K Means"),
           algorithm = recode(algorithm,
                              rp = "Random Proj.",
                              ica = "ICA",
                              pca = "PCA",
                              svd = "SVD"),
           title = paste(clusterer, ":", algorithm)) %>%
    ggplot(aes(x = n, y = k)) +
    geom_tile(aes(fill = adj_rand)) +
    facet_wrap(~ title, nrow = 2) +
    theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold")
      ) +
    labs(
      title = "Adjusted Rand Score For Different Hyperparameters",
      subtitle = "Lighter Tiles Indicate Higher Score.",
      x = "Number of Components",
      y = "Number of Clusters",
      color = NULL
      ) +
    guides(fill = FALSE) +
    scale_fill_gradient(low = "red4", high = "tomato2")
```

&nbsp;

```{r, fig.width=6, fig.height=3.2, message=FALSE, warning=FALSE, fig.align="center"}
sds <- dim_reduction_adult %>%
    filter(k == 2) %>%
    select(algorithm, clusterer, n, k, with_labels_sd, without_labels_sd) %>%
    gather(score_type, std, with_labels_sd:without_labels_sd) %>%
    mutate(score_type = recode(score_type,
                               without_labels_sd = "without_labels",
                               with_labels_sd = "with_labels"))

means <- dim_reduction_adult %>%
    filter(k == 2) %>%
    select(algorithm, clusterer, n, k, with_labels_mean, without_labels_mean) %>%
    gather(score_type, mean, with_labels_mean:without_labels_mean) %>%
    mutate(score_type = recode(score_type,
                             without_labels_mean = "without_labels",
                             with_labels_mean = "with_labels")) %>%
    left_join(sds, by = c("algorithm", "clusterer", "score_type", "n"))

means %>%
    filter(n < 2 | n > 5) %>%
    mutate(
       bottom = mean - 1.96 * std,
       top = mean + 1.96 * std,
       clusterer = recode(clusterer,
                          em = "EM",
                          kmeans = "K Means"),
       algorithm = recode(algorithm,
                          rp = "RP",
                          ica = "ICA",
                          pca = "PCA",
                          svd = "SVD"),
       title = paste(clusterer, ":", algorithm),
       score_type = recode(score_type,
                           with_labels = "With Labels",
                           without_labels = "Without Labels")
    ) %>%
    ggplot(aes(x = n)) +
    geom_ribbon(aes(ymin = bottom, ymax = top, fill = score_type), alpha = 0.1) +
    geom_line(aes(y = mean, color = score_type)) +
    geom_point(aes(y = mean, color = score_type)) +
    facet_wrap(~ title, nrow = 2) +
    theme_tq() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold")
      ) +
    labs(
      title = "Neural Network Performance With And Without Labels",
      x = "Number of Components",
      y = "Accuracy",
      color = NULL,
      fill = NULL
      )
```

&nbsp;

```{r, fig.width=6, fig.height=3.2, message=FALSE, warning=FALSE, fig.align="center"}
dim_reduction_adult %>%
    arrange(n, k) %>%
    mutate(n = as.factor(n), k = as.factor(k),
           clusterer = recode(clusterer,
                              em = "EM",
                              kmeans = "K Means"),
           algorithm = recode(algorithm,
                              rp = "Random Proj.",
                              ica = "ICA",
                              pca = "PCA",
                              svd = "SVD"),
           title = paste(clusterer, ":", algorithm)) %>%
    ggplot(aes(x = n, y = k)) +
    geom_tile(aes(fill = with_labels_mean)) +
    facet_wrap(~ title, nrow = 2) +
    theme_minimal() +
    theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        text = element_text(size = 10, family = "CMU Serif"),
        plot.title = element_text(face = "bold")
      ) +
    labs(
      title = "Cross Validation Score for Different Hyperparameters",
      subtitle = "With Cluster Labels As Feature",
      x = "Number of Components",
      y = "Number of Clusters",
      color = NULL
      ) +
    guides(fill = FALSE)
```









